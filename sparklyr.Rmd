---
title: "Untitled"
output: html_document
date: "2022-06-08"
---

This recipe deploys a Spark cluster on the GKE autopilot cluster. Most likely, you will have to run `spark_connect` twice. The first run of `spark_connect` can fail because it is likely to take longer than the timeout to download the Spark image to the GKE autopilot cluster. It is difficult to predict whether the Spark image has been previously downloaded to the GKE autopilot cluster or not. Hence, I set the timeout shorter than the default to start Spark quickly if the image already exists or run `spark_connect` again after you download the image in the first attempt.

Open a new terminal in the RStudio Server to load `~/keys/gcloud-auth.sh`.

Using the terminal in this RStudio Server,

- Type `kubectl cluster-info` to get the 'master' address for the 'spark_config_kubernetes'.

```{r}
library(sparklyr)
conf <- spark_config_kubernetes(
  master = "k8s://https://34.122.136.10", # Type kubectl cluster-info to get this url.
  version = "3.3",
  image = "us-central1-docker.pkg.dev/project-lee-1/docker/spark:3.3.0",
  driver = "sparklyr",
  account = "spark-admin", # Use the system account created by install-gcloud.sh with the NAMESPACE environment variable from compose.yml.
  jars = "local:///opt/sparklyr",
  executors = 10,
  timeout = 20 
)
conf$sparklyr.shell.conf <- c( # The following settings should be here to be used for spark-submit.
  conf$sparklyr.shell.conf,
  "spark.kubernetes.namespace=spark", # Use the namespace created by install-gcloud.sh  with the NAMESPACE environment variable from compose.yml.
  "spark.driver.cores=4",
  "spark.driver.memory=16g",
  "spark.executor.cores=1",
  "spark.executor.memory=4g"
)
```

```{r}
sc <- spark_connect(
  config = conf,
  spark_home = "/opt/spark/3.3.0" # Use the folder where Spark was installed by spark.sh
)
```

If the `spark_connect` above fails, do the following and then run the cell above again.

Using the terminal in this RStudio Server,

- Type `kubectl get all` several times until the container is 'running'.
- Type `kubectl delete pods --all` to delete all running container.

```{r}
connection_is_open(sc)
```

Go to a terminal on the virtual machine and press "shift + ` +  c" and then -L 4040:localhost:4040 to request local forward for 4040.

Type `kubectl port-forward pod/sparklyr 4040:4040 -n spark` on the virtual machine.

Then go to `localhost:4040` in the web browser on the local machine to access Spark Web UI.

```{r}
tbl_mtcars <- copy_to(sc, mtcars, "spark_mtcars")
```

```{r}
partitions <- tbl_mtcars %>%
  select(mpg, wt, cyl) %>%
  sdf_random_split(training = 0.5, test = 0.5, seed = 1099)
```

```{r}
fit <- partitions$training %>%
  ml_linear_regression(mpg ~ .)

fit
```

```{r}
summary(fit)
```

```{r}
spark_disconnect(sc)
```

Using the terminal in this RStudio Server, 

- Type `kubectl delete pods --all` to delete all pods in the namespace.

Using the terminal on the virtual machine,

- Press `ctrl` + `c` to stop port-forwarding to the service using `kubectl`.

- `-KL 4040` to stop local forward from the local machine to the virtual machine.
