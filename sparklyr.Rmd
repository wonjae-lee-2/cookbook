---
title: "Untitled"
output: html_document
date: "2022-06-08"
---

This recipe deploys a Spark cluster on the GKE autopilot cluster. Most likely, you will have to run `spark_connect` twice. The first run of `spark_connect` can fail because it is likely to take longer than the timeout to download the Spark image to the GKE autopilot cluster. It is difficult to predict whether the Spark image has been previously downloaded to the GKE autopilot cluster or not. Hence, I set the timeout shorter than the default to start Spark quickly if the image already exists. If it does not, run `spark_connect` again after you download the image in the first attempt.

Open a new terminal in the RStudio Server to load `~/keys/gcloud-auth.sh`.

Using the terminal in this RStudio Server,

- Type `kubectl cluster-info` to get the 'master' address for the 'spark_config_kubernetes'.

```{r}
library(sparklyr)
conf <- spark_config_kubernetes(
  master = "k8s://https://34.123.27.98", # Type kubectl cluster-info to get this url.
  version = "3.3",
  image = "us-central1-docker.pkg.dev/project-lee-1/docker/spark:3.3.0",
  driver = "sparklyr",
  account = "spark-admin", # Use the non-default system account for the current namespace set by the NAMESPACE environment variable from compose.yml and `gcloud-auth.sh`.
  jars = "local:///opt/sparklyr",
  executors = 10,
  timeout = 20
)
conf$sparklyr.shell.conf <- c( # The following settings should be here to be used for spark-submit.
  conf$sparklyr.shell.conf,
  "spark.kubernetes.namespace=spark", # Use the current namespace set by the NAMESPACE environment variable from compose.yml and `gcloud-auth.sh`.
  "spark.kubernetes.driver.request.cores=4",
  "spark.kubernetes.driver.limit.cores=4",
  "spark.driver.memory=16g",
  "spark.kubernetes.executor.request.cores=1",
  "spark.kubernetes.executor.limit.cores=1",
  "spark.executor.memory=4g"
)
```

```{r}
sc <- spark_connect(
  config = conf,
  spark_home = "/opt/spark/3.3.0" # Use the folder where Spark was installed by spark.sh
)
```

If the `spark_connect` above fails, do the following and then run the cell above again.

Using the terminal in this RStudio Server,

- Type `kubectl wait pod/sparklyr --for=condition=Ready --timeout=600s` to wait until the container is running.
- Type `kubectl get all` to verify the container is running.
- Type `kubectl delete pods --all` to delete all running container.

```{r}
connection_is_open(sc)
```

Type `kubectl port-forward pod/sparklyr 4040:4040 -n spark` on WSL.

Then go to `localhost:4040` in the web browser on the local machine to access Spark Web UI.

```{r}
tbl_mtcars <- copy_to(sc, mtcars, "spark_mtcars")
```

```{r}
partitions <- tbl_mtcars %>%
  select(mpg, wt, cyl) %>%
  sdf_random_split(training = 0.5, test = 0.5, seed = 1099)
```

```{r}
fit <- partitions$training %>%
  ml_linear_regression(mpg ~ .)

fit
```

```{r}
summary(fit)
```

```{r}
spark_disconnect(sc)
```

Using the terminal in this RStudio Server, 

- Type `kubectl delete pods --all` to delete all pods in the namespace.

Using the WSL terminal,

- Press `ctrl` + `c` to stop port-forwarding to the driver pod using `kubectl`.
